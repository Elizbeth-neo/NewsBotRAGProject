{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should be True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Print the index of the current GPU\n",
    "print(torch.cuda.get_device_name(0))  # Print the name of the GPU"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T08:45:13.808194Z",
     "end_time": "2024-09-05T08:45:16.461089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:00:10.796073Z",
     "end_time": "2024-09-05T09:00:12.138119Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "098b60ae1adf4151b332ede7adb2ac61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\")\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=ACCESS_TOKEN)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             token=ACCESS_TOKEN)\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:00:13.617608Z",
     "end_time": "2024-09-05T09:00:21.685526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekurochkina\\IdeaProjects\\NLPTasksProject_env_store\\LentaNewsRAGProjct\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to real-time information and cannot provide the latest news. For the most up-to-date news and updates, I recommend checking reputable news sources or search engines.\n"
     ]
    }
   ],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Tell me you didn't get the context and can't answer the question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a Russian-speaking news correspondent. People will ask you questions about the news.\n",
    "        Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "        1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "        2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "        3. Give the answer on Russian.\n",
    "        Context: {context}.\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer.encode(\n",
    "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = response[len(formatted_prompt) :]\n",
    "    response = response.replace(\"<eos>\", \"\")\n",
    "    return response\n",
    "\n",
    "question = \"Tell me the latest news\"\n",
    "print(inference(question=question, context=\"\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:01:01.061840Z",
     "end_time": "2024-09-05T09:01:04.220797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekurochkina\\AppData\\Local\\Temp2\\ipykernel_48012\\2837384794.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  huggingface_embeddings_ru = HuggingFaceEmbeddings(model_name='cointegrated/rubert-tiny2')\n",
      "C:\\Users\\ekurochkina\\IdeaProjects\\NLPTasksProject_env_store\\LentaNewsRAGProjct\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.read_csv('../lenta-ru-news-cropped.csv')\n",
    "loader = DataFrameLoader(news_df, page_content_column='text')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 700, chunk_overlap = 50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "huggingface_embeddings_ru = HuggingFaceEmbeddings(model_name='cointegrated/rubert-tiny2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:01:22.791924Z",
     "end_time": "2024-09-05T09:01:24.806314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local('../faiss_full_index_character_1k_ru', huggingface_embeddings_ru,  allow_dangerous_deserialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:01:46.077606Z",
     "end_time": "2024-09-05T09:01:46.108563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Около двух часов ночи с понедельника на вторник в жилой квартире на первом этаже дома 117 корпус 1 по Московскому проспекту в Петербурге возник пожар. Как сообщили РИА \"Новости\" со ссылкой на пресс-службу Главного управления государственной противопожарной службы МВД РФ, находившиеся в квартире двое мужчин и три женщины погибли, скорее всего, из-за отравления дымом. Огонь захватил площадь в 20 квадратных метров в кухне и коридоре. По информации ИТАР-ТАСС, тушением занимались 2 пожарных расчета. Через час огонь был локализован. На место прибыли три бригады \"скорой помощи\". Из горящей квартиры удалось выбраться только ее хозяйке. Причины пожара пока не установлены, отметили в пресс-службе МВД. По предварительным результатам расследования, пожар произошел из-за неосторожного обращения с огнем подвыпивших обитателей квартиры.\n",
      "Как сообщила телекомпания НТВ, 23 сентября около 15:50 по местному времени оперативному дежурному ГУВД Санкт-Петербурга поступила информация о взрыве в жилом доме на Васильевском острове. Адрес дома - Железноводская улица, 68. Утверждается, что взрыв произошел на лестничной клетке у лифта. Подробностей о причинах взрыва и характере повреждений пока не сообщается. По информации агентства ИТАР-ТАСС, взрыв произошел в 15:16 и к месту происшествия выехали сотрудники МЧС. Со ссылкой на главное управление по делам ГОЧС Санкт-Петербурга агентство сообщает, что жертв, разрушений и пожаров взрыв на Васильевской не причинил. \n",
      "Место происшествия - карта В.У. Сидыганова\n",
      "На газоперерабатывающем заводе в Нижневартовске в понедельник произошел взрыв сырого нефтяного газа, в результате чего возник сильный пожар. Как сообщает \"Новости\" со ссылкой на МВД РФ, выгорело около тысячи квадратных метров площади, огнем уничтожена компрессорная станция, восемь промышленных установок и агрегатов. Пострадавших нет. По данным МЧС РФ, на борьбу с огнем ушло более пяти часов. В настоящее время работа предприятия приостановлена, ведется расследование.\n",
      "Как сообщает агентство РИА \"Новости\", огромный пожар полыхает на северо-восточной окраине Старопромысловского района Грозного. Высота огня достигает от 50 до 100 метров. От ударов российской авиации загорелась газонефтераспределительная станция Грозненского нефтеперерабатывающего завода. Город постепенно заволакивает черный дым.\n",
      "Взрыв, эквивалентный 50 граммам тротила, произошелсегодня в 5.45 по московскому времени у фасада одного из магазиновпродовольственного комплекса на проспекте Ветеранов вКрасносельском районе Санкт-Петербурга. В пресс-центре местногоуправления ГОЧС корреспонденту ИТАР-ТАСС сообщили, что врезультате взрыва образовалась небольшая выемка, разбитовитринное стекло. Пострадавших нет. На месте происшествия работают сотрудники МЧС, ФСБ,патрульно-постовой службы, 42-го отделения милиции. Задержанподозреваемый в совершении взрыва.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Новость про пожар в Санкт-Петербурге\"\n",
    "retrieved_docs = vectorstore.similarity_search(question, k=5)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:01:47.857629Z",
     "end_time": "2024-09-05T09:01:47.879442Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Около двух часов ночи с понедельника на вторник в жилом квартире на первом этаже дома 117 корпус 1 по Московскому проспекту в Петербурге возник пожар. Как сообщили РИА \"Новости\" со ссылкой на пресс-службу Главного управления государственной противопожарной службы МВД РФ, находившиеся в квартире двое мужчин и три женщины погибли, скорее всего, из-за отравления дымом.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(inference(question=question, context=context))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-05T09:01:56.850217Z",
     "end_time": "2024-09-05T09:02:00.705065Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
